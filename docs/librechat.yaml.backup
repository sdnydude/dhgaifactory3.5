# DHG AI Factory - LibreChat Configuration
# Complete configuration file - no placeholders
version: 1.1.6
cache: true

# =============================================================================
# INTERFACE SETTINGS
# =============================================================================
interface:
  endpointsMenu: true
  modelSelect: true
  parameters: true
  sidePanel: true
  presets: true

# =============================================================================
# REGISTRATION & AUTH
# =============================================================================
registration:
  socialLogins: []
  allowedDomains: []

# =============================================================================
# RATE LIMITS
# =============================================================================
rateLimits:
  fileUploads:
    ipMax: 100
    ipWindowInMinutes: 60
    userMax: 50
    userWindowInMinutes: 60
  conversationsImport:
    ipMax: 100
    ipWindowInMinutes: 60
    userMax: 50
    userWindowInMinutes: 60

# =============================================================================
# ENDPOINTS
# =============================================================================
endpoints:
  # ---------------------------------------------------------------------------
  # OLLAMA (Local LLMs on .251)
  # ---------------------------------------------------------------------------
  ollama:
    # Models auto-detected from OLLAMA_BASE_URL in docker-compose
    titleModel: "mistral-small3.1:24b"
    summarize: true
    summaryModel: "mistral-small3.1:24b"
    forcePrompt: false
    modelDisplayLabel: "Ollama"

  # ---------------------------------------------------------------------------
  # CUSTOM ENDPOINTS (DHG Backend)
  # ---------------------------------------------------------------------------
  custom:
    # DHG Orchestrator - Main CME Pipeline
    - name: "DHG Orchestrator"
      apiKey: "${DHG_API_KEY}"
      baseURL: "http://dhg-aifactory-orchestrator:8000/v1"
      models:
        default:
          - "dhg-cme-pipeline"
          - "dhg-research"
          - "dhg-medical-llm"
          - "dhg-curriculum"
          - "dhg-outcomes"
          - "dhg-qa-compliance"
        fetch: false
      titleConvo: true
      titleModel: "dhg-cme-pipeline"
      summarize: false
      dropParams:
        - "user"
        - "frequency_penalty"
        - "presence_penalty"

    # DHG Research Agent - Direct Access
    - name: "Research Agent"
      apiKey: "${DHG_API_KEY}"
      baseURL: "http://dhg-research:8000/v1"
      models:
        default:
          - "research-pubmed"
          - "research-clinical-trials"
          - "research-cdc"
        fetch: false
      titleConvo: true
      summarize: false

    # DHG Medical LLM - Direct Access
    - name: "Medical LLM"
      apiKey: "${DHG_API_KEY}"
      baseURL: "http://dhg-medical-llm:8000/v1"
      models:
        default:
          - "medllama2"
          - "mistral-medical"
        fetch: false
      titleConvo: true
      summarize: false

  # ---------------------------------------------------------------------------
  # CLOUD PROVIDERS
  # ---------------------------------------------------------------------------
  anthropic:
    # Uses ANTHROPIC_API_KEY from .env
    models:
      default:
        - "claude-haiku-4-5-20251015"
        - "claude-sonnet-4-5-20250929"
      fetch: true

  openAI:
    # Uses OPENAI_API_KEY from .env
    models:
      default:
        - "gpt-4-turbo"
        - "gpt-4o"
      fetch: true

  google:
    # Uses GOOGLE_API_KEY from .env
    models:
      default:
        - "gemini-1.5-pro"
        - "gemini-1.5-flash"
      fetch: true

# =============================================================================
# MCP SERVERS (Model Context Protocol)
# =============================================================================
mcpServers:
  # DHG Agents as MCP Tools
  dhg-visuals:
    type: sse
    url: "http://dhg-visuals-media:8000/mcp"
    timeout: 60000

  dhg-transcribe:
    type: sse
    url: "http://dhg-transcribe:8200/mcp"
    timeout: 120000

  dhg-prompt-checker:
    type: sse
    url: "http://dhg-aifactory-orchestrator:8000/mcp/prompt-checker"
    timeout: 30000

# =============================================================================
# FILE HANDLING
# =============================================================================
fileConfig:
  endpoints:
    default:
      fileLimit: 10
      fileSizeLimit: 25
      totalSizeLimit: 100
      supportedMimeTypes:
        - "image/jpeg"
        - "image/png"
        - "image/gif"
        - "image/webp"
        - "application/pdf"
        - "text/plain"
        - "text/markdown"
        - "audio/mpeg"
        - "audio/wav"
        - "video/mp4"

# =============================================================================
# SPEECH (Text-to-Speech / Speech-to-Text)
# =============================================================================
speech:
  tts:
    openai:
      voices:
        - "alloy"
        - "echo"
        - "fable"
        - "onyx"
        - "nova"
        - "shimmer"
      model: "tts-1"

  stt:
    openai:
      model: "whisper-1"

# =============================================================================
# MODEL SPECS (Override defaults)
# =============================================================================
modelSpecs:
  enforce: false
  prioritize: true
  list:
    - name: "dhg-cme-pipeline"
      label: "DHG CME Pipeline"
      description: "Full CME content generation workflow"
      iconURL: "/assets/dhg-icon.png"
      default: true
      
    - name: "mistral-small3.1:24b"
      label: "Mistral Small 3.1 (24B)"
      description: "Local Mistral model via Ollama"
      
    - name: "medllama2"
      label: "MedLlama2"
      description: "Medical-specialized local LLM"
